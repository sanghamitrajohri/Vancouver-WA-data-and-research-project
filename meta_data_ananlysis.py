# -*- coding: utf-8 -*-
"""Meta_Data_Ananlysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mCRKJfGbJi39N3qDitGnhTQEb4vM0UAL

Zipcode vs Electricity consumption
"""

import pandas as pd

# Load the CSV file into a pandas DataFrame
file_path = '/content/building_metadata.csv'
building_data = pd.read_csv(file_path)
# Now let's filter the data to only include entries with SubstationID 'STN07' and count the unique zip codes associated with it.
stn07_data = building_data[building_data['SubstationID'] == 'STN07']
unique_zip_codes = stn07_data['Zip'].nunique()
# Let's group the data by Zip and count the frequency of each zip code for 'STN07'
zip_code_frequency = stn07_data['Zip'].value_counts()

# Display the zip codes and their frequencies
print(zip_code_frequency)

# Load the data from the CSV file
file_path = '/content/cleaned_load_data_without_stn38.csv'  # Replace with the correct file path
load_data = pd.read_csv(file_path)
# Now that we have the load data, let's sum the total consumption for STN07.
total_consumption_stn07 = load_data['STN07'].sum()

total_consumption_stn07
# Convert the 'Time' column to datetime format
load_data['Time'] = pd.to_datetime(load_data['Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')

# Extract the year from the 'Time' column
load_data['Year'] = load_data['Time'].dt.year

# Group by year and calculate the total consumption for STN07
yearly_consumption_stn07 = load_data.groupby('Year')['STN07'].sum()

# Display the result
print(yearly_consumption_stn07)
zip_code_frequency = stn07_data['Zip'].value_counts()

# Calculate consumption per zip code based on frequency
zip_code_consumption = (zip_code_frequency / zip_code_frequency.sum()) * total_consumption_stn07
print(zip_code_consumption)

# Calculate the total electricity consumption for each substation (STN)
substation_columns = [col for col in load_data.columns if col.startswith('STN')]
substation_consumption = load_data[substation_columns].sum()

# Convert the results to regular float format
substation_consumption_formatted = substation_consumption.apply(lambda x: '{:,.2f}'.format(x))

# Calculate the total sum of all STNs
total_consumption_all_stn = substation_consumption.sum()
# Print the formatted results and the total sum
substation_consumption_formatted, total_consumption_all_stn
# To provide the total electricity consumption for each STN on a yearly basis, we will group the data by both the year and the substations.

# Extract the relevant substation columns and group by the 'Year' and calculate the sum for each STN per year
yearly_substation_consumption = load_data.groupby(load_data['Year'])[substation_columns].sum()

# Format the numbers in the table to avoid scientific notation
yearly_substation_consumption_formatted = yearly_substation_consumption.applymap('{:,.2f}'.format)

#Calculate total electricity consumption for each substation (STN)
substation_consumption_total = load_data[substation_columns].sum()

stn_zip_code_consumption = {}

# Loop through each substation (STN)
for stn in substation_columns:
    # Filter metadata to only include the current substation
    stn_metadata = building_data[building_data['SubstationID'] == stn]

    # Calculate the frequency of ZIP codes within this substation
    zip_code_frequency = stn_metadata['Zip'].value_counts()

    # Calculate total consumption for the substation
    total_consumption = substation_consumption_total[stn]

    # Distribute the total consumption based on ZIP code frequencies
    zip_code_consumption = (zip_code_frequency / zip_code_frequency.sum()) * total_consumption

    # Store the results
    stn_zip_code_consumption[stn] = zip_code_consumption

# Convert the dictionary into a dataframe for easier display
zip_code_consumption_df = pd.concat(stn_zip_code_consumption, axis=1).fillna(0)

# Format the result for readability
zip_code_consumption_df_formatted = zip_code_consumption_df.applymap(lambda x: '{:,.2f}'.format(x))

#import ace_tools as tools; tools.display_dataframe_to_user(name="Electricity Consumption by Zip Code for All Substations", dataframe=zip_code_consumption_df_formatted)
# # Save the calculated zip code consumption values into a new CSV file
output_file_path = '/content/electricity_consumption_by_zip_and_stn.csv'
zip_code_consumption_df.to_csv(output_file_path)

output_file_path

# Load the uploaded file to examine its structure
file_path = '/electricity_consumption_by_zip_and_stn.csv'
data = pd.read_csv(file_path)
# Calculate the number of active stations (values not equal to 0) for each row
active_stations_count = data.iloc[:, 1:-2].astype(bool).sum(axis=1)

# Calculate the average consumption for each ZIP code by dividing the total consumption by the number of active stations
data['Average consumption'] = data['Total_consumption'] / active_stations_count

# Add the active stations count to the dataset for clarity
data['Active_stations'] = active_stations_count

# Save the updated dataset to a new CSV file
output_file_path = 'updated_electricity_consumption_by_zip_and_stn.csv.csv'
data.to_csv(output_file_path, index=False)

output_file_path

# Let's relabel the y-axis properly with the correct scale for electricity consumption.
import matplotlib.pyplot as plt
import pandas as pd

# Load the CSV file into a pandas DataFrame
file_path = '/content/updated_electricity_consumption_by_zip_and_stn.csv.csv'
uploaded_data = pd.read_csv(file_path)
plt.figure(figsize=(10,6))

# Plotting the bar chart with zip codes as categorical labels
plt.bar(uploaded_data['Zip'].astype(str), uploaded_data['Total_consumption'], color='skyblue')

plt.xlabel('Zip Code')
plt.ylabel('Total Electricity Consumption (in millions)')
plt.title('Electricity Consumption by Zip Code')

# Format the y-axis labels to show the correct scale (in millions)
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))

plt.xticks(rotation=90)
plt.tight_layout()

# Show the updated plot with properly formatted y-axis labels
plt.show()

"""Income per Capita by ZIP Code"""

# Load the data from the uploaded file
file_path = '/census2020.csv'
data = pd.read_csv(file_path)
# Calculate an approximation of income per capita
data['income_per_capita'] = data['median_income'] / data['total_population']

# Round the income per capita to three decimal places
data['income_per_capita (USD)'] = data['income_per_capita'].round(3)

# Import display settings
pd.set_option('display.max_rows', None)
#print(data[['zip_code', 'median_income', 'total_population', 'income_per_capita (USD)']])

# Save the result to a new CSV if needed
data.to_csv('income_per_capita_calculated.csv', index=False)
# 1. Bar plot for Income per Capita by ZIP Code

plt.figure(figsize=(12, 6))
sns.barplot(x='zip_code', y='income_per_capita', data=data.sort_values('income_per_capita', ascending=False), palette="viridis")
plt.xticks(rotation=90)
plt.title('Income per Capita by ZIP Code')
plt.xlabel('ZIP Code')
plt.ylabel('Income per Capita (USD)')
plt.show()

"""Calculating the average consumption per capita for each ZIP code"""

# Assuming "Average" represents total consumption for the ZIP code
data['consumption_per_capita'] = data['electricity_consumption'] / data['total_population']

#Calculate average consumption per capita
data['consumption_per_capita'] = data['electricity_consumption'] / data['total_population']

#Display the result
average_consumption_per_capita = data[['zip_code', 'consumption_per_capita']]
# Count the number of properties in each ZIP code
property_counts_by_zip = building_data['Zip'].value_counts()

# Check if 98601 has the most properties
most_properties_zip = property_counts_by_zip.idxmax()
most_properties_count = property_counts_by_zip.max()

# Compare with ZIP code 98601
properties_in_98601_count = property_counts_by_zip.get(98601, 0)
most_properties_zip, most_properties_count, properties_in_98601_count
# Plot average consumption per capita for each ZIP code as individual entities
plt.figure(figsize=(12, 6))
plt.bar(data['zip_code'].astype(str), data['consumption_per_capita'], color='skyblue')  # Convert ZIP code to string for better categorical display
plt.xlabel('ZIP Code')
plt.ylabel('Average Consumption Per Capita')
plt.title('Average Consumption Per Capita by ZIP Code')
plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability
plt.tight_layout()
# Show the plot
plt.show()

"""Consumption Per Capita vs. Median Income by ZIP Code"""

# Import necessary libraries
import matplotlib.pyplot as plt

# Create a scatter plot for Consumption Per Capita vs Median Income
plt.figure(figsize=(12, 8))
plt.scatter(data['median_income'], data['consumption_per_capita'], color='purple', s=100, alpha=0.7)

# Add labels to the points
for i, zip_code in enumerate(data['zip_code']):
    plt.text(data['median_income'][i], data['consumption_per_capita'][i], str(zip_code), fontsize=9, ha='right')

# Set labels and title
plt.xlabel('Median Income')
plt.ylabel('Consumption Per Capita')
plt.title('Consumption Per Capita vs. Median Income by ZIP Code')
plt.grid(True)

# Show the plot
plt.show()

"""Population-Consumption Density (Consumption per Capita)"""

census_data = pd.read_csv('/census2020.csv')
census_data['pop_consumption_density'] = census_data['electricity_consumption'] / census_data['total_population']

# Sort data by Population-Consumption Density for better visualization
sorted_data = census_data.sort_values(by='pop_consumption_density', ascending=False)

# Plot
plt.figure(figsize=(12, 8))
plt.bar(sorted_data['zip_code'].astype(str), sorted_data['pop_consumption_density'], color='skyblue')
plt.title('Population-Consumption Density by Zip Code')
plt.xlabel('Zip Code')
plt.ylabel('Population-Consumption Density (Consumption per Capita)')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Filter the data for the three ZIP codes of interest
racial_data = census_data[census_data['zip_code'].isin(zip_codes)]

# Select relevant columns for analysis
columns_of_interest = [
    'zip_code',
    'total_population',
    'population_white_alone',
    'population_black_or_african_american_alone',
    'population_american_indian_and_alaska_native_alone',
    'population_asian_alone',
    'population_native_hawaiian_and_other_pacific_islander_alone',
    'population_some_other_race_alone',
    'population_two_or_more_races',
]
racial_data_selected = racial_data[columns_of_interest]

# Calculate the percentage of each racial group in each ZIP code
racial_data_percentages = racial_data_selected.copy()
for col in columns_of_interest[2:]:
    racial_data_percentages[col] = (
        racial_data_selected[col] / racial_data_selected['total_population'] * 100
    )

# Reorder the ZIP codes manually
desired_order = [98683, 98662, 98601]
racial_data_percentages = racial_data_percentages.set_index('zip_code').loc[desired_order].reset_index()

# Define the racial categories and their simplified labels
categories = [
    'population_white_alone',
    'population_black_or_african_american_alone',
    'population_american_indian_and_alaska_native_alone',
    'population_asian_alone',
    'population_native_hawaiian_and_other_pacific_islander_alone',
    'population_some_other_race_alone',
    'population_two_or_more_races',
]

category_labels = [
    'White',
    'Black/African American',
    'American Indian/Alaska Native',
    'Asian',
    'Native Hawaiian/Pacific Islander',
    'Other Race',
    'Two or More Races',
]
# Re-plot with the corrected order
fig, ax = plt.subplots(figsize=(10, 6))

x = np.arange(len(desired_order))  # X-axis positions for ZIP codes
width = 0.12  # Width of each bar

# Loop through each racial category and plot
for i, (category, label) in enumerate(zip(categories, category_labels)):
    ax.bar(
        x + i * width,
        racial_data_percentages[category],
        width,
        label=label,
        edgecolor='black',
    )

# Formatting the chart
ax.set_title('Racial Composition by ZIP Code (%)', fontsize=16)
ax.set_xlabel('ZIP Codes', fontsize=14)
ax.set_ylabel('Percentage (%)', fontsize=14)
ax.set_xticks(x + width * (len(categories) - 1) / 2)
ax.set_xticklabels(desired_order, fontsize=12)
ax.legend(title='Racial Groups', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)

# Adjust layout for neatness
plt.tight_layout()
plt.show()

"""**Understanding** **the Building data**"""

import pandas as pd
import matplotlib.pyplot as plt

# Load your dataset into a DataFrame
# Replace 'your_dataset.csv' with the actual path to your dataset
df = pd.read_csv('/content/building_metadata.csv')

# Check if the columns 'BldgType' and 'SubstationID' exist
if 'BldgType' not in df.columns or 'SubstationID' not in df.columns:
    raise ValueError("The dataset must contain 'BldgType' and 'SubstationID' columns.")

# Count the occurrences of each building type
building_type_counts = df['BldgType'].value_counts().head(10)

# Filter the dataset to include only the top 10 most common building types
top_10_bldg_types = df[df['BldgType'].isin(building_type_counts.index)]

# Group the data by SubstationID and BldgType, then count occurrences
substation_bldgtype_counts = top_10_bldg_types.groupby(['SubstationID', 'BldgType']).size().unstack()

# Plot the stacked bar chart for the relationship between Substation and the top 10 building types
substation_bldgtype_counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab20c')
plt.title('Relationship between Substation and Top 10 Building Types')
plt.ylabel('Count')
plt.xlabel('Substation')
plt.legend(title='Building Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Count the occurrences of each building type
building_type_counts = df['BldgType'].value_counts().head(10)

# Filter the dataset to include only the top 10 most common building types
top_10_bldg_types = df[df['BldgType'].isin(building_type_counts.index)]

# Group the data by FeederID and BldgType, then count occurrences for the top 10 building types
feeder_bldgtype_counts = top_10_bldg_types.groupby(['FeederID', 'BldgType']).size().unstack()

# Plot the stacked bar chart for the relationship between Feeder and the top 10 building types
feeder_bldgtype_counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab20c')
plt.title('Relationship between Feeder and Top 10 Building Types')
plt.ylabel('Count')
plt.xlabel('Feeder')
plt.legend(title='Building Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Group the data by SubstationID and Zip, then count occurrences
substation_zip_counts = df.groupby(['SubstationID', 'Zip']).size().unstack()

# Plot the stacked bar chart for the relationship between Substations and Zip codes
substation_zip_counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab20c')
plt.title('Relationship between Substations and Zip Codes')
plt.ylabel('Count')
plt.xlabel('Substation')
plt.legend(title='Zip Code', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Group by SubstationID and Zip, then count occurrences
substation_zip_counts = df.groupby(['SubstationID', 'Zip']).size().reset_index(name='Count')
# Display all rows in a Jupyter Notebook or similar environment
from IPython.display import display
# display(substation_zip_counts)
# Calculate the total count of zip codes for each SubstationID
total_counts_by_substation = substation_zip_counts.groupby('SubstationID')['Count'].sum().reset_index()
# print(total_counts_by_substation)

# Filter data for STN01
stn01_data = substation_zip_counts[substation_zip_counts['SubstationID'] == 'STN01']

# Calculate the total count for STN01
total_count_stn01 = stn01_data['Count'].sum()

# Calculate the electricity consumption for each Zip code in STN01
stn01_data['Consumption'] = (stn01_data['Count'] / total_count_stn01) * 260605.95

# Load the CSV file into a pandas DataFrame
file_path = '/content/total_consumption.csv'
df = pd.read_csv(file_path)

# Load the uploaded building metadata file
metadata_file_path = '/content/building_metadata.csv'
metadata_df = pd.read_csv(metadata_file_path)

# Attempting the merge again after cleaning
merged_df = pd.merge(df, metadata_df, left_on='Station', right_on='SubstationID')
# Calculate the number of times each Zip appears for each Station
zip_counts = merged_df.groupby(['Station', 'Zip']).size().reset_index(name='Zip_Count')

# Merge the counts back to the merged dataframe
merged_with_counts = pd.merge(merged_df, zip_counts, on=['Station', 'Zip'])

# Calculate the proportion of consumption allocated to each Zip for each Station
merged_with_counts['Allocated Consumption'] = merged_with_counts['Total Consumption (mW)'] / merged_with_counts['Zip_Count']

# Group by Zip and sum the allocated consumption
corrected_zip_consumption_allocated = merged_with_counts.groupby('Zip')['Allocated Consumption'].sum().reset_index()

# Check the corrected consumption values
corrected_zip_consumption_allocated_total = corrected_zip_consumption_allocated['Allocated Consumption'].sum()

# Display the corrected data and total
corrected_zip_consumption_allocated, corrected_zip_consumption_allocated_total
sum_of_allocated_consumption = corrected_zip_consumption_allocated['Allocated Consumption'].sum()
sum_of_allocated_consumption

# Group the data by FeederID and Zip, then count occurrences
feeder_zip_counts = df.groupby(['FeederID', 'Zip']).size().unstack()

# Plot the stacked bar chart for the relationship between Feeders and Zip codes
feeder_zip_counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab20c')
plt.title('Relationship between Feeders and Zip Codes')
plt.ylabel('Count')
plt.xlabel('Feeder')
plt.legend(title='Zip Code', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Finding the most common building types
common_building_types = data['BldgType'].value_counts().head(10)
# Displaying the most common building types
common_building_types

# Filtering the data to include only building sizes between 0 and 2000 sqft
filtered_data = data[(data['BldgSqft'] >= 0) & (data['BldgSqft'] <= 2000)]

# 1. Histogram of Building Sizes (0 to 2000 sqft)
plt.figure(figsize=(12, 6))
plt.hist(filtered_data['BldgSqft'].dropna(), bins=50, color='skyblue', edgecolor='black')
plt.title('Histogram of Building Sizes (0 to 2000 sqft)', fontsize=16)
plt.xlabel('Building Size (sqft)', fontsize=14)
plt.ylabel('Number of Properties', fontsize=14)
plt.grid(axis='y', linestyle='--', color='gray', linewidth=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Plotting the histogram with a logarithmic scale
plt.figure(figsize=(12, 6))
plt.hist(data['BldgSqft'].dropna(), bins=np.logspace(0, np.log10(data['BldgSqft'].max()), 50), color='skyblue', edgecolor='black', alpha=0.7)
plt.xscale('log')

# Titles and labels
plt.title('Histogram of Building Sizes (Log Scale)', fontsize=16)
plt.xlabel('Building Size (sqft)', fontsize=14)
plt.ylabel('Number of Properties', fontsize=14)
plt.grid(True, linestyle='--', color='gray', linewidth=0.5)

# Show plot with tight layout
plt.tight_layout()
plt.show()

# Define the bin size
bin_size = 1000

# Define the bin edges
bin_edges = range(0, int(data['BldgSqft'].max()) + bin_size, bin_size)

# Create a new column 'BldgSqftRange' by binning the 'BldgSqft' values
data['BldgSqftRange'] = pd.cut(data['BldgSqft'], bins=bin_edges)

# Group by 'BldgSqftRange' and count the number of properties in each range
property_counts = data.groupby('BldgSqftRange').size().reset_index(name='TotalProperties')

# Filter out rows where the count is 0
property_counts_filtered = property_counts[property_counts['TotalProperties'] > 0]

# Output the table to an Excel file
property_counts_filtered.to_excel('property_counts.xlsx', index=False)

import pandas as pd

# Load the Excel file
file_path = '/content/property_counts.xlsx'
excel_data = pd.ExcelFile(file_path)

# Check the sheet names to understand the structure of the file
sheet_names = excel_data.sheet_names
sheet_names


# Load the data from the first sheet
data = pd.read_excel(file_path, sheet_name='Sheet1')

# Display the first few rows to understand the structure and column names
data.head()

# Calculate the average building square footage range based on total properties
data['Midpoint'] = data['BldgSqftRange'].apply(lambda x: (int(x.split(',')[0][1:]) + int(x.split(',')[1][:-1])) / 2)
average_midpoint = (data['Midpoint'] * data['TotalProperties']).sum() / data['TotalProperties'].sum()

# Find the range in which the average property lies
average_range = data.iloc[(data['Midpoint'] - average_midpoint).abs().argmin()]['BldgSqftRange']
average_midpoint, average_range

from scipy.stats import gaussian_kde
# Sort the data to find the top 10 ranges by number of properties
top_10_data = data.sort_values(by='TotalProperties', ascending=False).head(10)

# Create an expanded dataset to approximate the distribution with the top 10 ranges
top_10_expanded_data = []
for index, row in top_10_data.iterrows():
    lower_bound = int(row['BldgSqftRange'].split(',')[0][1:])
    upper_bound = int(row['BldgSqftRange'].split(',')[1][:-1])
    top_10_expanded_data.extend([np.mean([lower_bound, upper_bound])] * row['TotalProperties'])

# Use kernel density estimation to plot a smooth curve with the top 10 data
top_10_density = gaussian_kde(top_10_expanded_data)
top_10_xs = np.linspace(min(top_10_expanded_data), max(top_10_expanded_data), 1000)
top_10_density.covariance_factor = lambda : 0.25
top_10_density._compute_covariance()

# Plot the top 10 filtered distribution
plt.figure(figsize=(10, 6))
plt.plot(top_10_xs, top_10_density(top_10_xs), label='Top 10 Density Estimate', color='green')
plt.axvline(x=average_midpoint, color='r', linestyle='--', label=f'Average: {average_midpoint:.2f} sqft')
plt.xlabel('Building Square Footage')
plt.ylabel('Density')
plt.title('Top 10 Density Distribution of Properties by Building Square Footage')
plt.legend()
plt.show()

#property count
plt.figure(figsize=(12, 6))
property_use_counts = data['PropertyUseClass'].value_counts().head(10)  # Top 10 property use classes
sns.barplot(x=property_use_counts.index, y=property_use_counts.values, palette='viridis')
plt.title('Top 10 Property Use Classes by Count')
plt.xlabel('Property Use Class')
plt.ylabel('Count of Properties')
plt.xticks(rotation=45)
plt.show()

# Counting the number of properties in each zip code
zip_code_counts = data['Zip'].value_counts()
# Finding the zip code with the most properties
most_properties_zip = zip_code_counts.idxmax()
most_properties_count = zip_code_counts.max()

most_properties_zip, most_properties_count

"""The zip code with the most properties is 98682, which has a total of 18,488 properties."""

# 8. Property Use Class Distribution by County
# Creating a pivot table to count the occurrences of each property use class by county
property_use_county = pd.crosstab(data['CountyName'], data['PropertyUseClass'])

# Plotting the distribution using a stacked bar chart
plt.figure(figsize=(15, 8))
property_use_county.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis')
plt.title('Property Use Class Distribution by County')
plt.xlabel('County Name')
plt.ylabel('Count of Property Use Classes')
plt.legend(title='Property Use Class', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Create a bar chart to visualize the distribution of SubstationID
substation_counts = data['SubstationID'].value_counts()
plt.figure(figsize=(10, 6))
plt.bar(substation_counts.index, substation_counts.values, color='skyblue')
plt.xlabel('SubstationID')
plt.ylabel('Count')
plt.title('Distribution of SubstationID')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Generate random data points following a normal distribution based on the substation counts
mean = substation_counts.mean()
std_dev = substation_counts.std()
normal_data = np.random.normal(mean, std_dev, len(substation_counts))

# Plot the normal distribution of substation counts
plt.figure(figsize=(10, 6))
sns.histplot(normal_data, kde=True, color='skyblue', bins=10)
plt.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')
plt.axvline(mean + std_dev, color='green', linestyle='dashed', linewidth=1, label=f'Standard Deviation: {std_dev:.2f}')
plt.axvline(mean - std_dev, color='green', linestyle='dashed', linewidth=1)
plt.xlabel('Substation Count')
plt.ylabel('Frequency')
plt.title('Normal Distribution of Substation Counts')
plt.legend()
plt.tight_layout()
plt.show()

"""Count the occurrences of each FeederID"""

substation_counts = data['FeederID'].value_counts()
# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(substation_counts, labels=substation_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of FeederID')
plt.axis('equal')
plt.tight_layout()  # To prevent overlapping of labels
plt.show()

"""This below code is designed to:

  1.  Visualize geographic data: Display ZIP code-level information (e.g., income, population, race) on an interactive map.

  2. Analyze spatial patterns: Easily identify relationships between ZIP codes and demographic characteristics
"""

#! pip install geopandas folium
# import folium
# import geopandas as gpd
# from geopy.geocoders import Nominatim
# from folium.plugins import MarkerCluster

# # Initialize map focused on a broad region (using approximate coordinates for Washington state)
# map_center = [47.751076, -120.740135]  # Approximate center of Washington state
# spatial_map = folium.Map(location=map_center, zoom_start=7)

# # Geolocator to get lat-long of zip codes
# geolocator = Nominatim(user_agent="geoapiExercises")

# # Add markers for each ZIP code in the dataset with income and race proportions
# for index, row in census_data.iterrows():
#     zip_code = row['zip_code']
#     median_income = row['median_income']
#     total_population = row['total_population']
#     population_white = row['population_white_alone']
#     population_black = row['population_black_or_african_american_alone']
#     population_asian = row['population_asian_alone']

#     # Get lat-long for the ZIP code
#     location = geolocator.geocode(f"{zip_code}, WA")
#     if location:
#         # Add a marker for this ZIP code
#         popup_text = (f"ZIP: {zip_code}<br>"
#                       f"Median Income: ${median_income}<br>"
#                       f"Total Population: {total_population}<br>"
#                       f"White: {population_white}<br>"
#                       f"Black: {population_black}<br>"
#                       f"Asian: {population_asian}<br>")
#         folium.CircleMarker(
#             location=[location.latitude, location.longitude],
#             radius=5,  # Adjust radius based on population
#             popup=popup_text,
#             color='blue',  # Color can be adjusted to reflect different demographics
#             fill=True,
#             fill_opacity=0.7
#         ).add_to(spatial_map)

# # Save the map to an HTML file
# map_file_path = '/mnt/data/zip_code_income_race_map.html'
# spatial_map.save(map_file_path)

# map_file_path  # Return the file path for download

# # Step 1: Calculate percentage of White and Black population for each ZIP code
# data['percent_white'] = (data['population_white_alone'] / data['total_population']) * 100
# data['percent_black'] = (data['population_black_or_african_american_alone'] / data['total_population']) * 100

# # Step 2: Plotting
# fig, ax1 = plt.subplots(figsize=(14, 8))

# # Plot Consumption and Median Income
# ax1.set_xlabel('ZIP Code')
# ax1.set_ylabel('Consumption Per Capita & Median Income', color='blue')
# ax1.bar(data['zip_code'].astype(str), data['consumption_per_capita'], color='skyblue', label='Consumption Per Capita')
# ax1.plot(data['zip_code'].astype(str), data['median_income'], color='blue', marker='o', label='Median Income', linestyle='dashed')
# ax1.tick_params(axis='y', labelcolor='blue')

# # Second y-axis for race percentage
# ax2 = ax1.twinx()
# ax2.set_ylabel('Percentage (%)', color='green')
# ax2.plot(data['zip_code'].astype(str), data['percent_white'], color='green', marker='x', label='% White', linestyle=':')
# ax2.plot(data['zip_code'].astype(str), data['percent_black'], color='orange', marker='^', label='% Black', linestyle=':')
# ax2.tick_params(axis='y', labelcolor='green')

# # Title and layout
# plt.title('Consumption Per Capita, Median Income, and Race Percentage by ZIP Code')
# fig.tight_layout()

# # Legends
# ax1.legend(loc='upper left')
# ax2.legend(loc='upper right')

# # Show the plot
# plt.xticks(rotation=90)
# plt.show()

# import geopandas as gpd
# import pandas as pd
# import folium

# # Load your census data (from your CSV)
# census_data = pd.read_csv('/content/census2020.csv')
# # Load the shapefile, allowing invalid geometries
# zip_shapefile = gpd.read_file('/content/drive/MyDrive/tl_2024_us_zcta520/tl_2024_us_zcta520.shp', ignore_geometry_errors=True)
# # Check for invalid geometries
# invalid_geometries = zip_shapefile[~zip_shapefile.is_valid]

# # If there are invalid geometries, try fixing them
# zip_shapefile['geometry'] = zip_shapefile['geometry'].buffer(0)

# # Ensure both columns (ZCTA5CE20 and zip_code) are of the same type (string)
# zip_shapefile['ZCTA5CE20'] = zip_shapefile['ZCTA5CE20'].astype(str)
# census_data['zip_code'] = census_data['zip_code'].astype(str)

# # Merge shapefile with census data based on ZIP code
# merged_data = zip_shapefile.merge(census_data, left_on='ZCTA5CE20', right_on='zip_code')

# # Create a folium map centered on Washington state
# map_center = [47.751076, -120.740135]  # Adjust coordinates as needed
# spatial_map = folium.Map(location=map_center, zoom_start=7)

# # Add a choropleth layer for median income
# folium.Choropleth(
#     geo_data=merged_data,
#     name='choropleth',
#     data=merged_data,
#     columns=['zip_code', 'median_income'],
#     key_on='feature.properties.ZCTA5CE20',
#     fill_color='YlGn',
#     fill_opacity=0.7,
#     line_opacity=0.2,
#     legend_name='Median Income by ZIP Code'
# ).add_to(spatial_map)

# # Save the map to an HTML file
# spatial_map.save('zip_code_income_race_map.html')

# from google.colab import files
# files.download('zip_code_income_race_map.html')

# # Create a folium map for White population
# race_map = folium.Map(location=map_center, zoom_start=7)

# # Add a choropleth layer for White population
# folium.Choropleth(
#     geo_data=merged_data,
#     name='White Population',
#     data=merged_data,
#     columns=['zip_code', 'population_white_alone'],
#     key_on='feature.properties.ZCTA5CE20',
#     fill_color='Blues',
#     fill_opacity=0.7,
#     line_opacity=0.2,
#     legend_name='White Population'
# ).add_to(race_map)

# # Add ZIP code labels
# for _, row in merged_data.iterrows():
#     folium.Marker(
#         location=[row['INTPTLAT20'], row['INTPTLON20']],
#         popup=f"ZIP Code: {row['ZCTA5CE20']}",
#         icon=folium.DivIcon(html=f'<div style="font-size: 10pt">{row["ZCTA5CE20"]}</div>')
#     ).add_to(race_map)

# # Save the map to an HTML file
# race_map.save('white_population_by_zip_code.html')

# # Create a folium map for Electricity Consumption
# electricity_map = folium.Map(location=map_center, zoom_start=7)

# # Add a choropleth layer for electricity consumption (using the 'average' column for consumption)
# folium.Choropleth(
#     geo_data=merged_data,
#     name='Electricity Consumption',
#     data=merged_data,
#     columns=['zip_code', 'average'],  # 'average' column contains electricity consumption data
#     key_on='feature.properties.ZCTA5CE20',
#     fill_color='OrRd',
#     fill_opacity=0.7,
#     line_opacity=0.2,
#     legend_name='Electricity Consumption by ZIP Code'
# ).add_to(electricity_map)

# # Add ZIP code labels
# for _, row in merged_data.iterrows():
#     folium.Marker(
#         location=[row['INTPTLAT20'], row['INTPTLON20']],  # Latitude and Longitude from shapefile
#         popup=f"ZIP Code: {row['ZCTA5CE20']}",
#         icon=folium.DivIcon(html=f'<div style="font-size: 10pt">{row["ZCTA5CE20"]}</div>')
#     ).add_to(electricity_map)

# # Save the map to an HTML file
# electricity_map.save('electricity_consumption_by_zip_code.html')

